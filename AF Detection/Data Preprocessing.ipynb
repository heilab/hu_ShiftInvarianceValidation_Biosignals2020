{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data/\"\n",
    "fs = 250\n",
    "note_list = ['00735','03665','04043','04936','05091','06453','08378','08405','08434','08455']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 records\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for i in os.listdir(data_root):\n",
    "    if not \".dat\" in i:continue\n",
    "    key = i.split(\".\")[0]\n",
    "    if key not in note_list:\n",
    "        keys.append(key)\n",
    "print(\"There are\",len(keys),\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split keys to train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['08219', '06426', '05261']\n",
      "['04746', '07879', '04126']\n",
      "['07162', '04908', '04048']\n",
      "['06995', '08215', '05121']\n",
      "['04015', '07859', '07910']\n"
     ]
    }
   ],
   "source": [
    "cross = 5\n",
    "num_keys_every_cross = int(len(keys)/cross)\n",
    "for cross_idx in range(cross):\n",
    "    test_keys = keys[cross_idx*num_keys_every_cross:(cross_idx+1)*num_keys_every_cross]\n",
    "    assert(len(test_keys)==num_keys_every_cross)\n",
    "    print(test_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check R peak Detection Annotation in .qrs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot R-peak detection for 08219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4960ebed76814b5cba229f35568d688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saved R-peak detection for 08219 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-93d56648987a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msignals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdsamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mann_RRI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qrs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtime_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/envs/Biosignals/lib/python3.6/site-packages/wfdb/io/annotation.py\u001b[0m in \u001b[0;36mrdann\u001b[0;34m(record_name, extension, sampfrom, sampto, shift_samps, pb_dir, return_label_elements, summarize_labels)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;31m# Get wfdb annotation fields from the file bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     (sample, label_store, subtype,\n\u001b[0;32m-> 1253\u001b[0;31m      chan, num, aux_note) = proc_ann_bytes(filebytes, sampto)\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;31m# Get the indices of annotations that hold definition information about\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/envs/Biosignals/lib/python3.6/site-packages/wfdb/io/annotation.py\u001b[0m in \u001b[0;36mproc_ann_bytes\u001b[0;34m(filebytes, sampto)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# Get the next label store value - it may indicate additional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;31m# fields for this annotation, or the values of the next annotation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0mcurrent_label_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilebytes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_label_store\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m59\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = \"./CheckQRS\"\n",
    "win_len = 10 #second\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "for key in keys:\n",
    "    save_folder = os.path.join(save_dir,key)\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    signals,fields = wfdb.rdsamp(os.path.join(data_root,key))\n",
    "    ann_RRI = wfdb.rdann(os.path.join(data_root,key),extension=\"qrs\").sample\n",
    "    plt.plot(signals[:,0])\n",
    "    time_axis = [i/fs for i in range(len(signals))]\n",
    "    start = 0\n",
    "    end = start + win_len\n",
    "    plt.plot(time_axis,signals[:,0])\n",
    "    plt.xlabel(\"Time(s)\")\n",
    "    \n",
    "    print(\"plot R-peak detection for\",key)\n",
    "    for idx in tqdm_notebook(ann_RRI[:100]):\n",
    "        plt.scatter(x=idx/fs,y=signals[idx,0],color=\"r\")\n",
    "    \n",
    "    while(end<=(ann_RRI[100+1])/fs):\n",
    "        plt.xlim(start,end)\n",
    "        start += win_len\n",
    "        end = start+win_len\n",
    "        plt.savefig(os.path.join(save_folder,\"start=\"+str(start)+\"s.png\"))\n",
    "    plt.clf()\n",
    "    print(\"saved R-peak detection for\",key,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Annotation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08219 {'(AFIB', '(N'}\n",
      "06426 {'(AFL', '(AFIB', '(N', '(J'}\n",
      "05261 {'(AFIB', '(N'}\n",
      "04746 {'(AFIB', '(N'}\n",
      "07879 {'(AFIB', '(N', '(J'}\n",
      "04126 {'(AFIB', '(N'}\n",
      "07162 {'(AFIB'}\n",
      "04908 {'(AFL', '(AFIB', '(N'}\n",
      "04048 {'(AFIB', '(N'}\n",
      "06995 {'(AFL', '(AFIB', '(N'}\n",
      "08215 {'(AFL', '(AFIB', '(N'}\n",
      "05121 {'(AFIB', '(N', '(J'}\n",
      "04015 {'(AFIB', '(N'}\n",
      "07859 {'(AFIB'}\n",
      "07910 {'(AFL', '(AFIB', '(N'}\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    ann = wfdb.rdann(os.path.join(data_root,key),extension=\"atr\")\n",
    "    print(key,set(ann.aux_note))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check shortest segment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08219 2226\n",
      "06426 381\n",
      "05261 848\n",
      "04746 1174\n",
      "07879 685\n",
      "04126 4269\n",
      "07162 8999826\n",
      "04908 883\n",
      "04048 4387\n",
      "06995 1239\n",
      "08215 12560\n",
      "05121 624\n",
      "04015 421\n",
      "07859 8999957\n",
      "07910 1695\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    ann = wfdb.rdann(os.path.join(data_root,key),extension=\"atr\")\n",
    "    seg_lengths = []\n",
    "    total_length = 10*3600*fs\n",
    "    if len(ann.sample)==1:\n",
    "        seg_lengths.append(total_length - ann.sample[0])\n",
    "    else:\n",
    "        for i,s in enumerate(ann.sample[:-1]):\n",
    "            seg_lengths.append(ann.sample[i+1]-s)\n",
    "    \n",
    "    print(key,min(seg_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2int(label):\n",
    "    if label==\"N\":\n",
    "        return 0\n",
    "    if label==\"AFIB\":\n",
    "        return 1\n",
    "    if label==\"AFL\":\n",
    "        return 2\n",
    "    if label== \"J\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(key,seg_len=5):\n",
    "    ann = wfdb.rdann(os.path.join(data_root,key),extension=\"atr\")\n",
    "    signals,fields = wfdb.rdsamp(os.path.join(data_root,key))\n",
    "    annotation_indics = ann.sample\n",
    "    annotation_labels = [label2int(i[1:]) for i in ann.aux_note]\n",
    "    last_index = 0\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    #special process when there is only one annotation\n",
    "    if(len(annotation_indics)==1):\n",
    "        long_seg = signals[annotation_indics[0]:]\n",
    "        label = annotation_labels[0]\n",
    "        start = 0\n",
    "        end = start+seg_len*fs\n",
    "        while(end<len(long_seg)):\n",
    "            short_seg = long_seg[start:end]\n",
    "            start += seg_len*fs\n",
    "            end = start + seg_len*fs\n",
    "            X.append(short_seg)\n",
    "            y.append(label)\n",
    "        X = np.array(X)\n",
    "        #y = to_categorical(np.array(y),num_classes=4)\n",
    "        y = np.array(y)\n",
    "        return X,y\n",
    "    \n",
    "    for i,idx in enumerate(annotation_indics):\n",
    "        long_seg = signals[last_index:idx]\n",
    "        label = annotation_labels[i]\n",
    "        last_index = idx\n",
    "        \n",
    "        #slice long_seg to short_seg\n",
    "        if len(long_seg)<seg_len*fs:continue\n",
    "        start = 0\n",
    "        end = start+seg_len*fs\n",
    "        while(end<len(long_seg)):\n",
    "            short_seg = long_seg[start:end]\n",
    "            start += seg_len*fs\n",
    "            end = start + seg_len*fs\n",
    "            X.append(short_seg)\n",
    "            y.append(label)\n",
    "    X = np.array(X)\n",
    "    #y = to_categorical(np.array(y),num_classes=4)\n",
    "    y=np.array(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_balanced_and_shuffle(X,y,num_calss=2):\n",
    "    one_hot_y = to_categorical(y,4)\n",
    "    \n",
    "    num_each_class = np.sum(one_hot_y,axis=0)\n",
    "    num = int(min(num_each_class[0],num_each_class[1]))\n",
    "    \n",
    "    balanced_X = []\n",
    "    balanced_y = []\n",
    "    for class_idx in range(num_calss):\n",
    "        indices = np.where(y==class_idx)\n",
    "        selected_X = X[indices]\n",
    "        selected_y = y[indices]\n",
    "        balanced_X.append( selected_X[:num] )\n",
    "        balanced_y.append( selected_y[:num])\n",
    "    X = np.concatenate(balanced_X,axis=0)\n",
    "    y = np.concatenate(balanced_y,axis=0)\n",
    "    rand_index = np.random.permutation(len(X))\n",
    "    X = X[rand_index]\n",
    "    y = to_categorical(y[rand_index],num_calss)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizaiton(X_train,X_test):\n",
    "    ch1 = X_train[:,:,0]\n",
    "    ch2 = X_train[:,:,1]\n",
    "    \n",
    "    scaler1 = MinMaxScaler()\n",
    "    scaler1.fit(ch1)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(ch2)\n",
    "    \n",
    "    X_train = np.transpose(np.array([scaler1.transform(ch1),scaler2.transform(ch2)]),[1,2,0])\n",
    "    X_test = np.transpose(np.array([scaler1.transform(X_test[:,:,0]),scaler2.transform(X_test[:,:,1])]),[1,2,0])\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['08219', '06426', '05261']\n",
      "(19002, 1250, 2)\n",
      "(19002, 2)\n",
      "(6330, 1250, 2)\n",
      "(6330, 2)\n",
      "saved at ./dataset/data-cross-0.pickle\n",
      "['04746', '07879', '04126']\n",
      "(16968, 1250, 2)\n",
      "(16968, 2)\n",
      "(8364, 1250, 2)\n",
      "(8364, 2)\n",
      "saved at ./dataset/data-cross-1.pickle\n",
      "['07162', '04908', '04048']\n",
      "(24376, 1250, 2)\n",
      "(24376, 2)\n",
      "(956, 1250, 2)\n",
      "(956, 2)\n",
      "saved at ./dataset/data-cross-2.pickle\n",
      "['06995', '08215', '05121']\n",
      "(16534, 1250, 2)\n",
      "(16534, 2)\n",
      "(8798, 1250, 2)\n",
      "(8798, 2)\n",
      "saved at ./dataset/data-cross-3.pickle\n",
      "['04015', '07859', '07910']\n",
      "(24448, 1250, 2)\n",
      "(24448, 2)\n",
      "(884, 1250, 2)\n",
      "(884, 2)\n",
      "saved at ./dataset/data-cross-4.pickle\n"
     ]
    }
   ],
   "source": [
    "cross = 5\n",
    "num_keys_every_cross = int(len(keys)/cross)\n",
    "save_dir = \"./dataset\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for cross_idx in range(cross):\n",
    "    save_path = os.path.join(save_dir,\"data-cross-\"+str(cross_idx)+\".pickle\")\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    test_keys = keys[cross_idx*num_keys_every_cross:(cross_idx+1)*num_keys_every_cross]\n",
    "    print(test_keys)\n",
    "    assert(len(test_keys)==num_keys_every_cross)\n",
    "    for key in keys:\n",
    "        X,y = generate_dataset(key)\n",
    "        if key in test_keys:\n",
    "            X_test.append(X)\n",
    "            y_test.append(y)\n",
    "        else:\n",
    "            X_train.append(X)\n",
    "            y_train.append(y)\n",
    "            \n",
    "    X_train = np.concatenate(X_train,axis=0)\n",
    "    y_train = np.concatenate(y_train,axis=0)\n",
    "    X_test = np.concatenate(X_test,axis=0)\n",
    "    y_test = np.concatenate(y_test,axis=0)\n",
    "    \n",
    "    X_train,y_train = make_balanced_and_shuffle(X_train,y_train)\n",
    "    X_test,y_test = make_balanced_and_shuffle(X_test,y_test)\n",
    "    \n",
    "    X_train,X_test = normalizaiton(X_train,X_test)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    dataset = {\"test_key\":test_keys,\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test}\n",
    "    with open(save_path,\"wb\") as f:\n",
    "        pickle.dump(obj=dataset,file=f)\n",
    "    print(\"saved at\",save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
